# Neural Networks: MLP Optimization & Regularization

## ğŸ“Œ Project Overview
This repository contains a comprehensive implementation of a **Multi-Layer Perceptron (MLP)** to classify handwritten digits from the **MNIST** dataset. The project focuses on the practical application of Neural Network foundations, specifically **Optimization** strategies and **Regularization** techniques to prevent overfitting.

##  Key Experiments
This project goes beyond basic implementation by rigorously testing hyperparameters:

### 1. Optimization Algorithms
Compared convergence speed and stability across:
- **SGD** (Stochastic Gradient Descent)
- **SGD + Momentum**
- **Adam** (Adaptive Moment Estimation)
- **AdamW** (Adam with Weight Decay)

### 2. Regularization Techniques
- **Dropout:** Analyzed rates of 0.0, 0.1, 0.3, and 0.5.
- **L2 Regularization:** Tested weight decay penalties.
- **Early Stopping:** Implemented to automatically halt training when validation loss plateaus.

### 3. Hyperparameter Tuning
- **Batch Sizes:** 32 vs 64 vs 128 vs 256.
- **Activation Functions:** ReLU, Tanh, Sigmoid, GELU.

## ğŸ“Š Results Summary
| Experiment | Best Configuration | Accuracy |
| :--- | :--- | :--- |
| **Best Model** | 2 Layers + Dropout (0.1) | **99.12%** |
| **Optimizer** | AdamW | Fast Convergence |
| **Batch Size** | 32 | Best Generalization |
| **Activation** | ReLU / GELU | Highest Accuracy |

> **Note:** The project includes a custom generalization test where the model successfully predicts a hand-drawn digit ('7') that was preprocessed with Gaussian Blur to match the training distribution.

## ğŸ“‚ Repository Structure
```text
Neural-Networks-MLP/
â”‚
â”œâ”€â”€ Neural_Networks_MLP_Optimization.ipynb  # The Master Notebook
â”œâ”€â”€ models/                                # Saved .keras models for all experiments
â”œâ”€â”€ results/                               # Generated plots and visualizations
â”‚   â”œâ”€â”€ activation_tests/                  # ReLU vs Tanh vs Sigmoid vs GELU
â”‚   â”œâ”€â”€ batch_tests/                       # Batch size impact (32â€“256)
â”‚   â”œâ”€â”€ loss_curves/                       # Dropout and Regularization plots
â”‚   â”œâ”€â”€ optimizer_tests/                   # SGD vs Adam comparisons
â”‚   â””â”€â”€ predictions/                       # Prediction samples and Custom Digit test
â”œâ”€â”€ submission/                            # Detailed theoretical analysis (Markdown)
â”œâ”€â”€ requirements.txt                       # Dependencies
â””â”€â”€ README.md                              # Documentation
```
ğŸ› ï¸ Installation & Usage
1. Clone the repository
git clone https://github.com/YOUR_USERNAME/Neural-Networks-MLP-Optimization
cd Neural-Networks-MLP-Optimization

2. Install dependencies
pip install -r requirements.txt

3. Run the notebook
Open Neural_Networks_MLP_Optimization.ipynb using VS Code or Jupyter Lab and run all cells.

Author
Abdallah Gamal
Digital Egypt Pioneers Initiative (DEPI)
